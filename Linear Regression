import torch
import torch.nn as nn
import torch.nn.functional as nf
import torch.optim as optim

torch.manual_seed(1)

x_train = torch.FloatTensor([[73,  80,  75],
                             [93,  88,  93],
                             [89,  91,  80],
                             [96,  98,  100],
                             [73,  66,  70]])
y_train = torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

# weights are initialized as zero
W = torch.zeros((x_train.shape[1], y_train.shape[1]), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

optimizer = optim.SGD([W, b], lr=1e-5)  # SGD is a method of Gredient Descent, lr = learning rate
nb_epochs = 1000    # the number of trials you want


for epoch in range(nb_epochs + 1):

    hypothesis = x_train.matmul(W) + b
    cost = torch.mean((hypothesis - y_train)**2)

    optimizer.zero_grad()    # Initialize the slope of the cost function
    cost.backward()          # Calculate the new slope for W and b
    optimizer.step()         # Multiply by the learning rate

    # To check the progress
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(
            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))
